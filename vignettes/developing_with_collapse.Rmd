---
title: "Developing with collapse"
subtitle: "Or: How do Code Efficiently in R"
author: "Sebastian Krantz"
date: "2024-12-27"
output: 
  rmarkdown::html_vignette:
    toc: true

vignette: >
  %\VignetteIndexEntry{developing with collapse}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


<style type="text/css">
pre {
  max-height: 500px;
  overflow-y: auto;
}

pre[class] {
  max-height: 500px;
}
</style>






## Introduction

*collapse* offers an integrated suite of C/C++-based statistical and data manipulation functions, many low-level tools for memory efficient programming, and a class-agnostic architecture that seamlessly supports vectors, matrices, and data.frame-like objects. These features make it an ideal backend for high-performance statistical packages. This vignette outlines some recommendations when developing with *collapse*. It is complementary to the earlier [blog post on programming with *collapse*](https://sebkrantz.github.io/Rblog/2020/09/13/programming-with-collapse/) which readers are highly recommended to consult as well. The vignette gives additional clarity along 3 basic points. 

But before I jump into those let me start with an important warning which is that, while *collapse* is conveniently globally configurable using the `set_collapse()` function, this function should NEVER be called in a package: These are global options that will also affect *collapse*'s behavior outside of your package. 

## Point 1: Be Minimalistic Regarding Computations

*collapse* supports different types of R objects (vectors, matrices, *data.frame*'s + variants) and it can perform grouped operations on them using different types of grouping information (plain vector(s), 'qG'^[Alias for quick-group.] objects, factors, 'GRP' objects, grouped or indexed data frames). In addition, grouping can be sorted or unsorted. A key for very efficient code is to use a minimalist combination of these and re-use the grouping information as much as possible. 

For example, suppose you have the simple task of aggregating an object `x` by groups using a grouping vector `g`. If this grouping is only needed once, it should be done using the internal grouping of `fsum()` without creating any external grouping objects, e.g., `fsum(x, g)` for aggregation and `fsum(x, g, TRA = "fill")` for expansion. The expansion case in particular is very efficient internally because it internally uses unsorted grouping. In general, these functions, apart from the default sorting upon aggregation, are pretty smart in converting your vector input `g` into the minimally required grouping information. 

In the aggregation case, we can improve performance by also using unsorted grouping, i.e., `fsum(x, qF(g, sort = FALSE))` or `fsum(x, qG(g, sort = FALSE), use.g.names = FALSE)` if the row-names are not needed. If it is known that `g` is a plain vector or first-appearance order of groups should be kept even if `g` is a factor, use `group(g)` instead of `qG(g, sort = FALSE)` for highest performance.^[`group()` directly calls a C-based hashing algorithm which works for all types of vectors and lists of vectors/data frames. Missing values are treated as distinct elements.]

Naturally, if the same operation is needed on multiple vectors/objects, save the factor/'qG' object and reuse it. In that case, if not using `group()` to create 'qG', add the argument `na.exclude = FALSE` to `qF()/qG()` to preclude repeated internal missing value checks. Always set `use.g.names = FALSE` if not needed, and, if your data has no missing values, add arguments `na.rm = FALSE` to statistical functions like `fsum()` for maximum performance.

Overall, note that factors/'qG' are efficient inputs for computations with basically all statistical and data transformation functions except for `fmedian()`, `fnth()`, `fmode()`, `fndistinct()`, and split-apply-combine operations using `BY()/gsplit()`.  

For repeated grouped operations involving these latter functions, it makes sense to create 'GRP' objects using `g = GRP(g)`. These objects are more expensive to create but provide complete information - see `?GRP`, in particular the 'Value' section, for details. If sorting is not needed, set `sort = FALSE`, and if aggregation or the unique groups/group names are not needed set `return.groups = FALSE`. 

Only in rare cases where repeated operations on grouped data frames/indexed panels are needed does it make sense to employ `fgroup_by()` or `findex_by()`. Likewise, functions like `fsummarise()/fmutate()` are essentially wrappers. For example 


```r
mtcars |>
  fgroup_by(cyl, vs, am) |>
  fsummarise(mpg = fsum(mpg),
             across(c(carb, hp, qsec), fmean))
#   cyl vs am   mpg     carb        hp     qsec
# 1   4  0  1  26.0 2.000000  91.00000 16.70000
# 2   4  1  0  68.7 1.666667  84.66667 20.97000
# 3   4  1  1 198.6 1.428571  80.57143 18.70000
# 4   6  0  1  61.7 4.666667 131.66667 16.32667
# 5   6  1  0  76.5 2.500000 115.25000 19.21500
# 6   8  0  0 180.6 3.083333 194.16667 17.14250
# 7   8  0  1  30.8 6.000000 299.50000 14.55000
```

Is the same as (`use = FALSE` abbreviates `use.g.names = FALSE`)


```r
g <- GRP(mtcars, c("cyl", "vs", "am"))

add_vars(g$groups,
  get_vars(mtcars, "mpg") |> fsum(g, use = FALSE),
  get_vars(mtcars, c("carb", "hp", "qsec")) |> fmean(g, use = FALSE)
)
#   cyl vs am   mpg     carb        hp     qsec
# 1   4  0  1  26.0 2.000000  91.00000 16.70000
# 2   4  1  0  68.7 1.666667  84.66667 20.97000
# 3   4  1  1 198.6 1.428571  80.57143 18.70000
# 4   6  0  1  61.7 4.666667 131.66667 16.32667
# 5   6  1  0  76.5 2.500000 115.25000 19.21500
# 6   8  0  0 180.6 3.083333 194.16667 17.14250
# 7   8  0  1  30.8 6.000000 299.50000 14.55000
```

Of course, nothing prevents you from using these wrappers - they are quite efficient - but if you want to change all inputs programmatically it makes sense to go down one level. 

In general, it is important to think carefully about how to vectorize expressions (using *collapse* or auxiliary packages) in a minimalistic and memory efficient way. You will find that you can craft very parsimonious and efficient code to solve complicated problems. 

For example, after merging multiple spatial datasets, I had some of the same features from multiple sources, and, unwilling to match features individually across sources, I decided to keep the richest source on each feature type and location. After creating a feature `importance` indicator comparable across sources, the deduplication expression ended up being a single line of the form: `fsubset(data, source == fmode(source, list(location, type), importance, "fill"))`, i.e., keep the importance-weighted mode source.  

If an effective *collapse* solution is not apparent, other packages may offer efficient solutions. Check out the [*fastverse*](https://fastverse.github.io/fastverse/) and its [suggested packages list](https://fastverse.github.io/fastverse/#suggested-extensions). For example if you want to efficiently replace multiple items in a vector, `kit::vswitch()/nswitch()` can be pretty magical. Also functions like `data.table::set()/rowid()` etc. are great, e.g., as I had it [recently](https://github.com/SebKrantz/collapse/issues/627): what is the *collapse* equivalent to a grouped `dplyr::slice_head(n)`? It would be `fsubset(data, data.table::rowid(id1, id2, ...) <= n)`.


## Point 2: Think About Memory and Optimize

R programs are inefficient for 2 principal reasons: (1) operations are not vectorized; (2) too many intermediate objects/copies are created. *collapse*'s vectorized statistical functions address (1) effectively, but it also provides many [efficient programming functions](https://sebkrantz.github.io/collapse/reference/efficient-programming.html) to deal with (2). 

A key source of inefficiency in R code is the widespread use of logical vectors. For example 


```r
x <- abs(round(rnorm(1e6)))
x[x == 0] <- NA
```

where `x == 0` creates a logical vector of 1 million elements just to indicate to R which elements of `x` are `0`. In *collapse*, `setv(x, 0, NA)` is the efficient equivalent. This also works if we don't want to replace with `NA` but with another vector y, i.e. 


```r
y <- rnorm(1e6)
setv(x, NA, y) # Replaces missing x with y
```
is much better than 

```r
x[is.na(x)] <- y[is.na(x)]
```
`setv()` is quite versatile and also works with indices and logical vectors instead of elements to search for, and it can invert the query using the `invert = TRUE` argument. 

In more complex workflows, we may want to save the logical vector, e.g. `xmiss <- is.na(x)` and use it repeatedly. One aspect to note here is the logical vectors are inefficient for subsetting compared to indices: 


```r
xNA <- na_insert(x, prop = 0.4)
xmiss <- is.na(xNA)
ind <- which(xmiss) # Create directly using whichNA(xNA)
bench::mark(x[xmiss], x[ind])
# # A tibble: 2 × 6
#   expression      min   median `itr/sec` mem_alloc `gc/sec`
#   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>
# 1 x[xmiss]     3.13ms   3.55ms      267.    8.39MB     98.0
# 2 x[ind]     782.81µs 953.43µs     1053.    3.05MB     96.2
```

Thus, indices are always preferable. With *collapse*, they can be created directly using `whichNA(xNA)` in this case, or using `whichv(x, 0)` for `which(x == 0)` or any other number. Also here there exist an `invert = TRUE` argument covering the `!=` case. To facilitate programming, operators `x %==% 0` and `x %!=% 0` exist as well wrapping `whichv(x, 0)` and `whichv(x, 0, invert = TRUE)`, respectively. 

Similarly, the function `fmatch()` supports faster matching with associated operators `%iin%` and `%!iin%` which also return indices, e.g., `letters %iin% c("a", "b")` returns `1:2`. This can also be used in subsetting: 


```r
bench::mark(
  `%in%` = fsubset(wlddev, iso3c %in% c("USA", "DEU", "ITA", "GBR")),
  `%iin%` = fsubset(wlddev, iso3c %iin% c("USA", "DEU", "ITA", "GBR"))
)
# # A tibble: 2 × 6
#   expression      min   median `itr/sec` mem_alloc `gc/sec`
#   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>
# 1 %in%        144.2µs    163µs     6010.     3.8MB     25.3
# 2 %iin%        16.9µs     22µs    42984.   130.4KB     81.8
```

Likewise, `anyNA(), allNA(), anyv()` and `allv()` exist to avoid expressions like `any(x == 0)` in favor of `anyv(x, 0)`. Other convenience functions exist such as `na_rm(x)` for the common `x[!is.na(x)]` expression which is extremely inefficient. 

Another hint here particularly for data frame subsetting is the function `ss()`, which has an argument `check = FALSE` to avoid checks on indices:


```r
ind <- wlddev$iso3c %!iin% c("USA", "DEU", "ITA", "GBR")
microbenchmark::microbenchmark(
  withcheck = ss(wlddev, ind),
  nocheck = ss(wlddev, ind, check = FALSE)
)
# Unit: microseconds
#       expr    min       lq     mean   median       uq      max neval
#  withcheck 56.908 107.8915 240.9312 113.8365 171.6875 5753.899   100
#    nocheck 54.407 106.5795 128.9987 111.1715 170.8060  283.064   100
```

Another common source of inefficiencies is copy on modification in statistical operations. For example 


```r
x <- rnorm(100); y <- rnorm(100); z <- rnorm(100)
res <- x + y + z # Creates 2 copies
```
For this particular case `res <- kit::psum(x, y, z)` offers a direct efficient solution^[In general, also see other packages, in particular *kit* and *data.table* for useful programming functions.]. A more general solution is 

```r
res <- x + y
res %+=% z
```
*collapse*'s `%+=%`, `%-=%`, `%*=%` and `%/=%` operators are wrappers around the function `setop()` and also work with matrices and data frames.^[*Note* that infix operators do not obey the rules of arithmetic but are always evaluated from left to right.] This function also supports a `rowwise` argument for operations between vectors and matrix/data.frame rows, e.g.


```r
m <- qM(mtcars)
setop(m, "*", seq_col(m), rowwise = TRUE)
head(m / qM(mtcars))
#                   mpg cyl disp hp drat wt qsec  vs  am gear carb
# Mazda RX4           1   2    3  4    5  6    7 NaN   9   10   11
# Mazda RX4 Wag       1   2    3  4    5  6    7 NaN   9   10   11
# Datsun 710          1   2    3  4    5  6    7   8   9   10   11
# Hornet 4 Drive      1   2    3  4    5  6    7   8 NaN   10   11
# Hornet Sportabout   1   2    3  4    5  6    7 NaN NaN   10   11
# Valiant             1   2    3  4    5  6    7   8 NaN   10   11
```
Beyond `setop()` and some functions like `na_locf()/na_focb()` have `set = TRUE` arguments to perform operations by reference. There is also the function `setTRA()` for grouped transformations by reference, which wraps `TRA(..., set = TRUE)`. Since `TRA` is added as an argument to all [*Fast Statistical Functions*](https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html), `set = TRUE` can be passed down to modify by reference. For example: 


```r
fmedian(iris$Sepal.Length, iris$Species, TRA = "fill", set = TRUE)
```
Is the same as `setTRA(iris$Sepal.Length, fmedian(iris$Sepal.Length, iris$Species), "fill", iris$Species)`, replacing the values of the `Sepal.Length` vector with their species median by reference: 

```r
head(iris)
#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1            5         3.5          1.4         0.2  setosa
# 2            5         3.0          1.4         0.2  setosa
# 3            5         3.2          1.3         0.2  setosa
# 4            5         3.1          1.5         0.2  setosa
# 5            5         3.6          1.4         0.2  setosa
# 6            5         3.9          1.7         0.4  setosa
```
This `set` argument can be invoked anywhere, also inside `fmutate()` calls, and without groups. This can also be done in combination with other transformations (sweeping operations). For example the following turns the columns of the matrix into proportions.


```r
fsum(m, TRA = "/", set = TRUE)
fsum(m) # Check
#  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb 
#    1    1    1    1    1    1    1    1    1    1    1
```

So in summary, think what is really needed to complete a task and keep things to a minimum in terms of both computations and memory. Let's do a final exercise in this regard and create a hyper-efficient function for univariate linear regression by groups: 


```r
greg <- function(y, x, g) {
  g <- group(g)
  dmx <- fmean(x, g, TRA = "-", na.rm = FALSE)
  (fsum(y, g, dmx, use.g.names = FALSE, na.rm = FALSE) %/=% 
   fsum(dmx, g, dmx, use.g.names = FALSE, na.rm = FALSE))
}

# Test 
y <- rnorm(1e7)
x <- rnorm(1e7)
g <- sample.int(1e6, 1e7, TRUE)

microbenchmark::microbenchmark(greg(y, x, g), group(g))
# Unit: milliseconds
#           expr      min       lq      mean    median        uq      max neval
#  greg(y, x, g) 128.2287 135.5552 158.71251 144.04464 158.90448 267.1353   100
#       group(g)  60.6766  63.7885  75.41054  69.02596  78.62383 180.1109   100
```

The expression computed by `greg()` amounts to `sum(y * (x - mean(x)))/sum((x - mean(x))^2)` for each group, which is equal to `cov(x, y)/var(x)`, but it is very efficient, requiring exactly one full copy of `x` to create a group-demeaned vector, `dmx`, and then using the `w` (weights) argument to `fsum()` to sum the products (`y * dmx` and `dmx * dmx`) on the fly, including a division by reference avoiding an additional copy. One cannot do much better coding a grouped regression directly in C, and ostensibly the grouping step accounts for half of the computing time. 


## Point 3: Internally Favor Primitive R Objects and Functions

This partly reiterates Point 1 but now with a focus on internal data representation rather than grouping and computations. The point could also be bluntly stated as: 'vectors, matrices and lists are good, data frames and complex objects are bad'. 

Many frameworks seem to imply the opposite, e.g., the *tidyverse* encourages you to cast your data as a tidy tibble, and also *data.table* offers you a more efficient data.frame. But these objects are internally complex, and, in the case of *data.table*, only efficient because of some of the internal C-level algorithms for large-data manipulation. You should take a step back and ask yourself: For the statistical software I am writing, do I need this complexity? Complex objects require complex methods to manipulate them, thus, when using them, you incur the cost of everything that goes on in these methods. Vectors, matrices, and lists are much more efficient in R and *collapse* provides you many options to manipulate them directly. 

It may surprise you to hear that, internally, *collapse* does not use *data.frame*-like objects at all. Instead, such objects are cast to lists using `unclass()`, `class(data) <- NULL`, or `attributes(data) <- NULL` and code is written to address the list. This is advisable if you want to write fast package code. Of course there are limits, e.g., for advanced manipulations such as `join()` or `pivot()`, but many tasks can be solved using simpler objects. If you don't want to internally convert data.frame's to lists, you can use the functions `.subset()` and `.subset2()` to extract columns efficiently and `attr()` to extract/set attributes. Likewise for matrices you should use `dimnames()` directly instead of `rownames()` and `colnames()` which are wrappers around it.

The benchmark below illustrates that basically everything you do on the data.frame is more expensive than on the equivalent list. 


```r
l <- unclass(mtcars)
nam <- names(mtcars)
microbenchmark::microbenchmark(names(mtcars), attr(mtcars, "names"), names(l), 
               names(mtcars) <- nam, attr(mtcars, "names") <- nam, names(l) <- nam, 
               mtcars[["mpg"]], .subset2(mtcars, "mpg"), l[["mpg"]], 
               mtcars[3:8], .subset(mtcars, 3:8), l[3:8], 
               ncol(mtcars), length(mtcars), length(unclass(mtcars)), length(l), 
               nrow(mtcars), length(.subset2(mtcars, 1L)), length(l[[1L]]))
# Unit: nanoseconds
#                          expr  min     lq    mean median     uq   max neval
#                 names(mtcars)  164  246.0  340.30  246.0  287.0  3157   100
#         attr(mtcars, "names")   41   82.0  182.45   82.0  123.0  2419   100
#                      names(l)    0    0.0   57.40   41.0   41.0  1681   100
#          names(mtcars) <- nam  410  533.0  920.04  656.0  799.5  5699   100
#  attr(mtcars, "names") <- nam  287  369.0  828.61  492.0  615.0  9184   100
#               names(l) <- nam  205  246.0  756.04  266.5  328.0 33333   100
#               mtcars[["mpg"]] 2091 2419.0 3972.90 3649.0 4018.0 24108   100
#       .subset2(mtcars, "mpg")    0   41.0  128.74   82.0   82.0  4059   100
#                    l[["mpg"]]   41   41.0  107.01   82.0   82.0  1394   100
#                   mtcars[3:8] 5412 6949.5 9020.00 8097.5 9614.5 41041   100
#          .subset(mtcars, 3:8)  246  287.0  931.93  328.0  471.5 15826   100
#                        l[3:8]  246  307.5  732.67  369.0  492.0 10783   100
#                  ncol(mtcars) 1066 1189.0 1914.29 1312.0 2501.0 12177   100
#                length(mtcars)  164  246.0  378.02  246.0  307.5  4756   100
#       length(unclass(mtcars))  123  164.0  240.67  184.5  205.0  1558   100
#                     length(l)    0    0.0   68.47   20.5   41.0  3157   100
#                  nrow(mtcars) 1025 1189.0 1927.82 1332.5 2480.5  6068   100
#  length(.subset2(mtcars, 1L))   41   82.0  174.25  123.0  123.0  1435   100
#               length(l[[1L]])   41   82.0  143.50   82.0  123.0  1435   100
```


By means of further illustration, let's recreate the `pwnobs()` function in *collapse* which counts pairwise missing values. The list method is written in R. A basic implementation would be:^[By Point 2 this implementation is not ideal because I am creating two logical vectors for each iteration of the inner loop, but I currently don't see any way to write this more efficiently.]


```r
pwnobs_list <- function(X) {
    dg <- fnobs(X)
    n <- ncol(X)
    nr <- nrow(X)
    N.mat <- diag(dg)
    for (i in 1:(n - 1L)) {
        miss <- is.na(X[[i]])
        for (j in (i + 1L):n) N.mat[i, j] <- N.mat[j, i] <- nr - sum(miss | is.na(X[[j]]))
    }
    rownames(N.mat) <- names(dg)
    colnames(N.mat) <- names(dg)
    N.mat
}

mtcNA <- na_insert(mtcars, prop = 0.2)
pwnobs_list(mtcNA)
#      mpg cyl disp hp drat wt qsec vs am gear carb
# mpg   26  21   20 22   22 21   22 21 22   21   21
# cyl   21  26   22 21   21 22   20 21 20   22   20
# disp  20  22   26 21   20 22   20 22 21   21   22
# hp    22  21   21 26   23 22   21 22 21   21   23
# drat  22  21   20 23   26 20   22 22 22   20   21
# wt    21  22   22 22   20 26   20 21 20   22   21
# qsec  22  20   20 21   22 20   26 22 22   21   21
# vs    21  21   22 22   22 21   22 26 21   21   22
# am    22  20   21 21   22 20   22 21 26   21   21
# gear  21  22   21 21   20 22   21 21 21   26   21
# carb  21  20   22 23   21 21   21 22 21   21   26
```

Now with the above tips we can optimize this as follows:

```r
pwnobs_list_opt <- function(X) {
    dg <- fnobs.data.frame(X)
    class(X) <- NULL
    n <- length(X)
    nr <- length(X[[1L]])
    N.mat <- diag(dg)
    for (i in 1:(n - 1L)) {
        miss <- is.na(X[[i]])
        for (j in (i + 1L):n) N.mat[i, j] <- N.mat[j, i] <- nr - sum(miss | is.na(X[[j]]))
    }
    dimnames(N.mat) <- list(names(dg), names(dg))
    N.mat
}

identical(pwnobs_list(mtcNA), pwnobs_list_opt(mtcNA))
# [1] TRUE

microbenchmark::microbenchmark(pwnobs_list(mtcNA), pwnobs_list_opt(mtcNA))
# Unit: microseconds
#                    expr     min      lq      mean   median      uq     max neval
#      pwnobs_list(mtcNA) 150.757 153.381 157.04681 155.3900 159.982 202.171   100
#  pwnobs_list_opt(mtcNA)  26.322  27.142  29.39372  28.5565  31.119  38.950   100
```

As the benchmark shows, the optimized function is 6x faster on this (small) dataset, and we have changed nothing to the loop doing the computation. With larger data the difference would of course become less stark, but you never know what's going on in methods you have not written and how they behave with larger data. My advice is: avoid them, use simple objects and take full control of your code. This also makes your code more robust and you can create class-agnostic code. 

<!--
As another illustration let's say we want to aggregate an input-output table:


```r
IOT <- matrix(1, 260, 260)
nam <- as.vector(t(outer(LETTERS, 1:10, paste, sep = ".")))
dimnames(IOT) <- list(nam, nam)
IOT[1:11, 1:11]
#      A.1 A.2 A.3 A.4 A.5 A.6 A.7 A.8 A.9 A.10 B.1
# A.1    1   1   1   1   1   1   1   1   1    1   1
# A.2    1   1   1   1   1   1   1   1   1    1   1
# A.3    1   1   1   1   1   1   1   1   1    1   1
# A.4    1   1   1   1   1   1   1   1   1    1   1
# A.5    1   1   1   1   1   1   1   1   1    1   1
# A.6    1   1   1   1   1   1   1   1   1    1   1
# A.7    1   1   1   1   1   1   1   1   1    1   1
# A.8    1   1   1   1   1   1   1   1   1    1   1
# A.9    1   1   1   1   1   1   1   1   1    1   1
# A.10   1   1   1   1   1   1   1   1   1    1   1
# B.1    1   1   1   1   1   1   1   1   1    1   1

IOT_DF <- qDF(IOT, row.names.col = "country_sector") 
head(IOT_DF, 3)
#   country_sector A.1 A.2 A.3 A.4 A.5 A.6 A.7 A.8 A.9 A.10 B.1 B.2 B.3 B.4 B.5 B.6 B.7 B.8 B.9 B.10
# 1            A.1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1
# 2            A.2   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1
# 3            A.3   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1
#   C.1 C.2 C.3 C.4 C.5 C.6 C.7 C.8 C.9 C.10 D.1 D.2 D.3 D.4 D.5 D.6 D.7 D.8 D.9 D.10 E.1 E.2 E.3 E.4
# 1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1
# 2   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1
# 3   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1
#   E.5 E.6 E.7 E.8 E.9 E.10 F.1 F.2 F.3 F.4 F.5 F.6 F.7 F.8 F.9 F.10 G.1 G.2 G.3 G.4 G.5 G.6 G.7 G.8
# 1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1
# 2   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1
# 3   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1
#   G.9 G.10 H.1 H.2 H.3 H.4 H.5 H.6 H.7 H.8 H.9 H.10 I.1 I.2 I.3 I.4 I.5 I.6 I.7 I.8 I.9 I.10 J.1
# 1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1
# 2   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1
# 3   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1
#   J.2 J.3 J.4 J.5 J.6 J.7 J.8 J.9 J.10 K.1 K.2 K.3 K.4 K.5 K.6 K.7 K.8 K.9 K.10 L.1 L.2 L.3 L.4 L.5
# 1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1
# 2   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1
# 3   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1
#   L.6 L.7 L.8 L.9 L.10 M.1 M.2 M.3 M.4 M.5 M.6 M.7 M.8 M.9 M.10 N.1 N.2 N.3 N.4 N.5 N.6 N.7 N.8 N.9
# 1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1
# 2   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1
# 3   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1
#   N.10 O.1 O.2 O.3 O.4 O.5 O.6 O.7 O.8 O.9 O.10 P.1 P.2 P.3 P.4 P.5 P.6 P.7 P.8 P.9 P.10 Q.1 Q.2
# 1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1
# 2    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1
# 3    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1
#   Q.3 Q.4 Q.5 Q.6 Q.7 Q.8 Q.9 Q.10 R.1 R.2 R.3 R.4 R.5 R.6 R.7 R.8 R.9 R.10 S.1 S.2 S.3 S.4 S.5 S.6
# 1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1
# 2   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1
# 3   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1
#   S.7 S.8 S.9 S.10 T.1 T.2 T.3 T.4 T.5 T.6 T.7 T.8 T.9 T.10 U.1 U.2 U.3 U.4 U.5 U.6 U.7 U.8 U.9
# 1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1
# 2   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1
# 3   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1
#   U.10 V.1 V.2 V.3 V.4 V.5 V.6 V.7 V.8 V.9 V.10 W.1 W.2 W.3 W.4 W.5 W.6 W.7 W.8 W.9 W.10 X.1 X.2
# 1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1
# 2    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1
# 3    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1
#   X.3 X.4 X.5 X.6 X.7 X.8 X.9 X.10 Y.1 Y.2 Y.3 Y.4 Y.5 Y.6 Y.7 Y.8 Y.9 Y.10 Z.1 Z.2 Z.3 Z.4 Z.5 Z.6
# 1   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1
# 2   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1
# 3   1   1   1   1   1   1   1    1   1   1   1   1   1   1   1   1   1    1   1   1   1   1   1   1
#   Z.7 Z.8 Z.9 Z.10
# 1   1   1   1    1
# 2   1   1   1    1
# 3   1   1   1    1
```

We can aggregate this as a matrix or data.frame using *collapse* and `data.table::transpose()`:


```r

agg_IOT_DF <- function(IOT_DF) {
  IOT_DF |> 
    fmutate(country = substr(country_sector, 1, 1)) |>
    fgroup_by(country) |>
    num_vars() |> fsum() |>
    transpose(keep.names = "country_sector", make.names = "country") |>
    fmutate(country = substr(country_sector, 1, 1)) |>
    fgroup_by(country) |>
    num_vars() |> fsum() |>
    transpose(keep.names = "country", make.names = "country") 
}

agg_IOT <- function(IOT) {
  f <- qF(substr(dimnames(IOT)[[1L]], 1, 1), na.exclude = FALSE)
  IOT |> fsum(f) |> t() |> fsum(f) |> t()
}

identical(qM(agg_IOT_DF(IOT_DF), 1), agg_IOT(IOT))
# [1] TRUE
microbenchmark::microbenchmark(agg_IOT_DF(IOT_DF), agg_IOT(IOT))
# Unit: microseconds
#                expr     min      lq     mean  median       uq      max neval
#  agg_IOT_DF(IOT_DF) 523.119 537.264 570.7696 543.455 552.0855 3010.630   100
#        agg_IOT(IOT) 373.223 383.760 406.4728 386.835 389.8280 2211.663   100
```
-->

Further to this: it is quite easy to reconstruct the *data.frame* afterwards, e.g. 


```r
attr(l, "row.names") <- .set_row_names(length(l[[1L]]))
class(l) <- "data.frame"
head(l, 2)
#   mpg cyl disp  hp drat    wt  qsec vs am gear carb
# 1  21   6  160 110  3.9 2.620 16.46  0  1    4    4
# 2  21   6  160 110  3.9 2.875 17.02  0  1    4    4
```

You can also use *collapse* functions `qDF()`, `qDT()` and `qTBL()` to efficiently convert/create *data.frame*'s, *data.table*'s, and *tibble*'s:


```r
library(data.table)
library(tibble)
microbenchmark::microbenchmark(qDT(mtcars), as.data.table(mtcars), 
                               qTBL(mtcars), as_tibble(mtcars))
# Unit: microseconds
#                   expr    min      lq      mean  median      uq       max neval
#            qDT(mtcars)  3.034  3.7310   9.53250  4.5715  5.3300   482.734   100
#  as.data.table(mtcars) 38.089 42.6195  54.62348 44.8950 47.7240   916.555   100
#           qTBL(mtcars)  2.419  2.7470   5.60511  3.1980  4.2025   202.294   100
#      as_tibble(mtcars) 54.161 57.9945 569.62530 61.0490 64.4930 50633.278   100

l <- unclass(mtcars)
microbenchmark::microbenchmark(qDF(l), as.data.table(l), as_tibble(l))
# Unit: microseconds
#              expr    min     lq      mean  median       uq       max neval
#            qDF(l)  1.599  2.050   4.03358  2.2960   3.3620    64.001   100
#  as.data.table(l) 78.966 85.567 568.67861 90.0155 101.0855 45793.105   100
#      as_tibble(l) 63.017 66.543 539.75926 69.5770  81.2825 45338.538   100
```

Finally, *collapse* also provides functions like `setattrib()`, `copyMostAttrib()`, etc., to efficiently attach attributes again. So another efficient workflow for *data.frame*-like objects is to save the attributes `ax <- attributes(data)`, manipulate it as a list `l <- unclass(data)`, modify `ax$names` and `ax$row.names` as needed and then use `setattrib(l, ax)` to turn it into a *data.frame* again before returning it. This way your code will also work with *tibble* etc.

## Conclusion

*collapse* can become a game-changer for your statistical software development in R, enabling you to write programs that effectively run like C while accomplishing complex statistical/data tasks with few lines of code. This however requires taking a closer look at the package, in particular the [documentation](https://sebkrantz.github.io/collapse/reference/collapse-documentation.html), and following the three points given in this vignette. 

